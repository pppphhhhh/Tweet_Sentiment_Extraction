{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import pickle\n",
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 4)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_train(data_path,nrows=None):\n",
    "    df = pd.read_csv(data_path,nrows = nrows)\n",
    "    df['textID']=df['textID'].astype(str)\n",
    "    df['text']=df['text'].astype(str)\n",
    "    df['selected_text']=df['selected_text'].astype(str)\n",
    "    df['sentiment']=df['sentiment'].astype(str)\n",
    "    return df\n",
    "train_df = read_train(\"C:\\\\Users\\\\PANGPENGHUI\\\\Desktop\\\\ESG\\\\ESG5BD\\\\machine_learning\\\\tweet_tp3\\\\train.csv\",nrows=100)\n",
    "\n",
    "train_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_set = set()\n",
    "def get_words_index(documents,build=True, save=True, index_path= \"index.pickle\"):\n",
    "    '''\n",
    "    documents: pd.Series, string\n",
    "    '''\n",
    "    \n",
    "    tknzr = TweetTokenizer()    \n",
    "    def build_index(row):\n",
    "        global words_set\n",
    "        tokens = tknzr.tokenize(row.lower())\n",
    "        words_set = words_set.union(set(tokens))\n",
    "        return tokens\n",
    "    \n",
    "    if build:\n",
    "        documents.apply(build_index)\n",
    "        words_index = {k+1:v for k,v in enumerate(words_set)}\n",
    "        if save:            \n",
    "            with open(index_path, 'wb') as out:\n",
    "                pickle.dump(words_index, out)\n",
    "    \n",
    "    else:\n",
    "         words_index = pickle.load(out, encoding = \"utf-8\")\n",
    "                \n",
    "    return words_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'fb', 2: 'tweeple', 3: 'until', 4: 'better', 5: 'avid', 6: 'car', 7: 'drink', 8: 'constant', 9: 'keane', 10: 'thing', 11: 'put', 12: 'thats', 13: 'day', 14: 'back', 15: 'rebootiness', 16: 'foll', 17: '80', 18: 'those', 19: 'oh', 20: 'that', 21: 'account', 22: 'soooooo', 23: 'tomorrow', 24: 'went', 25: 'left', 26: '3d', 27: 'with', 28: 'feedings', 29: 'girl', 30: 'gone', 31: 'facebook', 32: 'like', 33: 'very', 34: 'irape', 35: 'one', 36: 'even', 37: 'the', 38: 'looking', 39: 'popped', 40: '/', 41: 'promise', 42: 'money', 43: 'past', 44: 'sharpie', 45: 'up', 46: 'boy', 47: 'school', 48: 'funny', 49: '?', 50: 'friends', 51: 'round', 52: 'few', 53: 'don', 54: 'haha', 55: 'high', 56: 'dent', 57: 'none', 58: 'smiles', 59: 'leave', 60: 'buy', 61: 'soooo', 62: 'how', 63: 'low', 64: 'would', 65: 'who', 66: 'vs', 67: 'backk', 68: 'updates', 69: 'gor-juz', 70: '½', 71: 'nyc', 72: 'prolly', 73: 'san', 74: 'bedtime', 75: 'morning', 76: 'pearcy', 77: 'us', 78: 'asleep', 79: 'mothers', 80: 'dissappointed', 81: ')', 82: 'coworker', 83: 'romance', 84: 'my', 85: 'comming', 86: 'runner', 87: 'plugging', 88: 'mayday', 89: 'want', 90: 'wine', 91: 'mums', 92: 'celtics-lakers', 93: 'sleeeeepy', 94: 'baddd', 95: 'good', 96: 'sleeping', 97: 'coming', 98: 'sorry', 99: 'win', 100: 'some', 101: 'n', 102: 'yesterday', 103: 'madd', 104: 'lasts', 105: 'minimal', 106: 'twit', 107: 'minute', 108: 'zero', 109: 'casey', 110: 'play', 111: '#hunchback', 112: 'tell', 113: 'talked', 114: 'alternatives', 115: 'last', 116: 'mum', 117: 'loserville', 118: 'old', 119: 'let', 120: 'bored', 121: 'restart', 122: 'sources', 123: 'its', 124: 'cake', 125: 'every', 126: 'ahhh', 127: 'http://bit.ly/2hpbg4', 128: 'living', 129: '-', 130: 'yyyyyyyyyoooooooooouuuuu', 131: 'go', 132: 'an', 133: 'sad', 134: 'http://plurk.com/p/rp3k7', 135: 'waiting', 136: 'piddled', 137: 'lose', 138: 'carpet', 139: 'job', 140: 'way', 141: 'theyre', 142: 'sooo', 143: 'all', 144: 'flu', 145: 'donbt', 146: 'power', 147: 'hair', 148: 'do', 149: 'shopping', 150: 'reply', 151: 'adding', 152: 'baby', 153: 'called', 154: 'asked', 155: 'game', 156: 'later', 157: 'miles', 158: 'wow', 159: 'other', 160: 'mannnn', 161: 'tired', 162: 'said', 163: 'warning', 164: 'being', 165: 'meal', 166: '_mounce', 167: 'tonite', 168: 'is', 169: 'hurts', 170: 'essex', 171: 'a', 172: 'unfortunately', 173: 'd', 174: 'shall', 175: 'lambert', 176: '2am', 177: 'sucked', 178: 'yes', 179: 'post', 180: 'malaysia', 181: 'fighting', 182: 'visiting', 183: 'he', 184: '10', 185: 'friidays', 186: '0w', 187: 'raised', 188: 'we', 189: 'boooooooooooo', 190: 'keeps', 191: 'ready', 192: 'voice', 193: 'but', 194: 'obesed', 195: 'into', 196: 'friend', 197: 'has', 198: 'testing', 199: 'u', 200: 'prydz', 201: 'anyone', 202: 'shameless', 203: 'salon', 204: 'him', 205: 'knee', 206: 'til', 207: 'dont', 208: 'guys', 209: 'spoil', 210: 'ace', 211: 'doing', 212: 'ride', 213: 'couldn', 214: 'coded', 215: 'ice', 216: 'magazine', 217: 'addicted', 218: 'sleep', 219: 'hate', 220: 'pity', 221: 'dear', 222: 'journey', 223: 'as', 224: 'she', 225: 'music', 226: 'bah', 227: 'etc', 228: 'guttah', 229: 'video', 230: 'became', 231: 'dragon', 232: 'smh', 233: 'uh', 234: 'kirin', 235: 'when', 236: 'hope', 237: 'say', 238: 'thrilled', 239: 'take', 240: 'be', 241: 'big', 242: 'supposed', 243: 'egh', 244: 'watch', 245: 'juss', 246: 'bag', 247: 'blog', 248: 'plenty', 249: 'mean', 250: 'adam', 251: 'soon', 252: 'sniffle', 253: 'still', 254: 'tha', 255: 'ï', 256: 'thank', 257: 'hes', 258: 'hemp', 259: 'really', 260: 'agent', 261: 'and', 262: 'family', 263: 'fun', 264: 'rather', 265: 'rad', 266: 'busy', 267: 'not', 268: 'going', 269: ',', 270: 'lost', 271: 'too', 272: 'me', 273: 'from', 274: 'nighty', 275: 'hi', 276: 'open', 277: 'simfinger', 278: 'mess', 279: 'earth', 280: 'make', 281: 'ok', 282: 'are', 283: 'head', 284: 'quite', 285: 'should', 286: 'wish', 287: 'minutes', 288: 'kick', 289: 'can', 290: '=p', 291: 'berkeleyy', 292: 'seen', 293: 'online', 294: 'stay', 295: 'eyebrows', 296: 'catch', 297: 'hero', 298: 'nice', 299: 'third', 300: 'your', 301: 'didn', 302: 'help', 303: 'wait', 304: 'vids', 305: 'kid', 306: 'wonder', 307: 'wars', 308: '2', 309: 'smacked', 310: 'hott', 311: 'wud', 312: 'it', 313: 'running', 314: 'wierd', 315: 'boo', 316: 'torn', 317: 'on', 318: 'easily', 319: 'think', 320: 'star', 321: 'consolation', 322: 'fran', 323: 'why', 324: 'looks', 325: 'then', 326: 'hearts', 327: 'read', 328: 'fillin', 329: 'm', 330: 'through', 331: 'love', 332: 'mine', 333: '.', 334: 'interesting', 335: 'unni', 336: 'our', 337: 'at', 338: 'chilliin', 339: 'stuff', 340: 'boss', 341: 'beta', 342: 'drunk', 343: 'before', 344: 'missin', 345: 'freistunde', 346: '<3', 347: 'you', 348: 'holiday', 349: 'hm', 350: 'access', 351: 'havent', 352: 'find', 353: 'dj', 354: 'get', 355: 'kno', 356: 'eating', 357: 'both', 358: 'out', 359: 'song', 360: 'lg', 361: 'know', 362: 'noida', 363: 'work', 364: 'off', 365: 'nashville', 366: 'best', 367: 'summer', 368: 'snoring', 369: 'peel', 370: 'rocked', 371: 't', 372: 'beers', 373: 'magazines', 374: 'tuned', 375: 'internet', 376: 'free', 377: 'gotta', 378: 'favorite', 379: 'house', 380: 'to', 381: 'alone', 382: 'test', 383: 'try', 384: 'eva', 385: 'home', 386: 'pics', 387: 'marly', 388: 'ticket', 389: 'least', 390: 'new', 391: 'connect', 392: 'thus', 393: 'bottle', 394: 'iphone', 395: 'uk', 396: 'more', 397: 'http://tinyurl.com/mnf4kw', 398: 'much', 399: 'whassqoodd', 400: 'eric', 401: 'than', 402: 'fam', 403: 'change', 404: 'only', 405: 'now', 406: 'baddie', 407: '...', 408: 'playing', 409: 'hoping', 410: 'blogs', 411: 'honestly', 412: 'by', 413: 'here', 414: 'aw', 415: 'was', 416: 'ink', 417: 'hey', 418: 'over', 419: 'll', 420: 'also', 421: 'metamorph', 422: 'check', 423: 'sick', 424: 'little', 425: 'week', 426: 's', 427: 'smell', 428: 'lol', 429: 'ny', 430: 'http://twittersucks.com', 431: 'rematch', 432: ':', 433: 'rangers', 434: 'kids', 435: 'ratt', 436: 'taylor', 437: 'didnt', 438: 'jaja', 439: 'response', 440: '`', 441: 'were', 442: 'graduation', 443: 'swift', 444: '..', 445: 'f0llowers', 446: 'probably', 447: 'about', 448: 'such', 449: 'army', 450: 'they', 451: 'arrive', 452: 'wat', 453: 'enjoy', 454: 'marvelous', 455: 've', 456: 'speak', 457: 'pop', 458: 'unhappy', 459: 'friendster', 460: 'see', 461: 'parody', 462: 'hehe', 463: 'cloth', 464: 'what', 465: \"'\", 466: 'texas', 467: 'friday', 468: 'freaked', 469: 'write', 470: 'have', 471: 'sons', 472: 'no', 473: 'where', 474: 'thought', 475: 'this', 476: 'problem', 477: 'audition', 478: 'yay', 479: 'today', 480: 'heavenly', 481: '_', 482: 'script', 483: 'releases', 484: '5pm', 485: '7', 486: 'bought', 487: 'flickr', 488: 'pills', 489: 'right', 490: 'show', 491: 'so', 492: 'crossing', 493: 'got', 494: 'cause', 495: 'reckon', 496: 'tons', 497: 'things', 498: 'never', 499: 'for', 500: 'boot', 501: 'any', 502: 'says', 503: 'im', 504: 'i', 505: 'speaking', 506: 'isn', 507: 'diego', 508: 'photoshoot', 509: 'just', 510: 'am', 511: 'fears', 512: 'chances', 513: 'available', 514: 'convert', 515: 'freelesson', 516: 'trying', 517: 'days', 518: 'cut', 519: 'annoying', 520: 'born', 521: 'coos', 522: 'jealous', 523: 'red', 524: 'sigh', 525: 'time', 526: 'blah', 527: 'extra', 528: 'working', 529: ';', 530: 'cares', 531: 'give', 532: 'though', 533: 'awesome', 534: 'jeje', 535: 'there', 536: 'http://bit.ly/ngnar', 537: 'early', 538: 'happy', 539: 'sleepy', 540: 'joined', 541: 'look', 542: 'his', 543: 'sunburned', 544: 'design', 545: 'fell', 546: 'sounds', 547: 'possible', 548: 'yea', 549: 'late', 550: 'dunno', 551: 'relax', 552: '!', 553: 'bby', 554: 'sm', 555: 'ghost', 556: 'suckkkkkk', 557: 'bad', 558: 'bed', 559: 'tears', 560: '#kitchenfire', 561: 'cool', 562: 'sweeeeet', 563: 'ipod', 564: 'trim', 565: 'env', 566: 'case', 567: 'bout', 568: 'everyone', 569: 'cream', 570: '*', 571: 'cleaning', 572: 'end', 573: 'slept', 574: 'omg', 575: 'wanna', 576: 'smoke', 577: 'cute', 578: 'story', 579: 'of', 580: 'cooler', 581: 'been', 582: 'came', 583: 'saw', 584: 'miss', 585: 'hat', 586: 'soggy', 587: 'feel', 588: 'encore', 589: 'ily', 590: 'bmi', 591: 'worked', 592: 'years', 593: 'fingers', 594: 'will', 595: 'o', 596: 'dangerously', 597: 'lucky', 598: 'bullying', 599: 'already', 600: 'well', 601: 'hangovers', 602: 'hopeful', 603: 'them', 604: 'app', 605: 'computer', 606: 'oz', 607: 'pet', 608: 'prawns', 609: 'hahaha', 610: 'in', 611: 'ran', 612: 'http://www.dothebouncy.com/smf', 613: 'twitter', 614: 'if', 615: 'crawling', 616: 'getting', 617: 'night', 618: 'dahye', 619: 'publicly', 620: 'plot', 621: 'tonight', 622: 'tested', 623: '(', 624: 'which', 625: 'run', 626: '¿', 627: 'guess', 628: 'interview', 629: 'gonna', 630: 'hospital', 631: 'fan', 632: 'her', 633: 'forum', 634: 'responded'}\n"
     ]
    }
   ],
   "source": [
    "words_index = get_words_index(train_df['text'], build=True, save=True)\n",
    "print(words_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'neutral', 1: 'negative', 2: 'positive'}\n"
     ]
    }
   ],
   "source": [
    "class_index = {k:v for k,v in enumerate(train_df['sentiment'].unique()) }\n",
    "\n",
    "print(class_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sequences (documents, index):\n",
    "    tknzr = TweetTokenizer()\n",
    "    sequences = []\n",
    "    for doc in documents:\n",
    "        sequences.append([index[w] for w in tknzr.tokenize(doc.lower())])\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[504, 440, 173, 470, 634, 269, 614, 504, 441, 268], [142, 133, 504, 594, 584, 347, 413, 610, 73, 507, 552, 552, 552], [84, 340, 168, 598, 272, 407], [464, 628, 552, 59, 272, 381], [471, 579, 570, 570, 570, 269, 323, 213, 440, 371, 450, 11, 603, 317, 37, 483, 188, 599, 486], [612, 129, 100, 202, 87, 499, 37, 366, 433, 633, 317, 279], [176, 28, 499, 37, 152, 282, 263, 235, 183, 168, 143, 58, 261, 521], [61, 55], [357, 579, 347], [222, 552, 49, 158, 407, 199, 509, 230, 580, 333, 462, 407, 623, 168, 20, 547, 552, 49, 81], [223, 398, 223, 504, 331, 380, 240, 602, 269, 504, 495, 37, 512, 282, 105, 290, 504, 440, 329, 498, 629, 354, 84, 124, 261, 339], [504, 259, 259, 32, 37, 359, 331, 578, 412, 436, 443], [84, 44, 168, 313, 596, 63, 317, 416], [504, 89, 380, 131, 380, 225, 621, 193, 504, 270, 84, 192, 333], [382, 382, 273, 37, 360, 565, 308], [233, 19, 269, 504, 510, 543], [426, 440, 281, 269, 516, 380, 620, 114, 223, 188, 456, 570, 524, 570], [504, 440, 455, 581, 423, 499, 37, 43, 52, 517, 261, 392, 269, 84, 147, 324, 314, 333, 614, 504, 437, 470, 171, 585, 317, 312, 64, 541, 407, 397], [168, 14, 385, 405, 629, 584, 125, 35], [257, 509, 267, 20, 195, 347], [19, 387, 269, 504, 440, 329, 491, 98, 552, 552, 504, 236, 347, 352, 632, 251, 552, 552, 346, 346], [408, 555, 293, 168, 259, 334, 333, 37, 390, 68, 282, 234, 607, 261, 421, 499, 299, 139, 333, 289, 440, 371, 303, 380, 470, 171, 231, 607], [168, 571, 37, 379, 499, 632, 262, 65, 168, 85, 156, 479, 444], [377, 121, 84, 605, 444, 504, 474, 99, 485, 415, 242, 380, 11, 132, 572, 380, 37, 8, 15], [460, 452, 504, 249, 567, 16, 186, 185, 407, 312, 440, 426, 153, 137, 445, 467, 407, 232], [37, 376, 328, 440, 604, 317, 84, 563, 168, 263, 269, 503, 217], [504, 440, 329, 98, 333], [317, 37, 140, 380, 180, 407, 472, 375, 350, 380, 106], [245, 582, 67, 273, 291, 529, 574, 123, 103, 263, 358, 535, 351, 581, 358, 535, 610, 171, 107, 333, 399, 49], [24, 380, 218, 261, 535, 168, 171, 146, 518, 610, 362, 146, 14, 45, 267, 528, 271], [504, 440, 329, 268, 385, 405, 333, 470, 347, 292, 84, 390, 613, 544, 49, 284, 407, 480, 506, 440, 570, 570, 570, 49], [504, 236, 335, 594, 280, 37, 477, 333, 181, 618, 335, 552], [614, 312, 168, 501, 321, 504, 493, 84, 590, 622, 609, 312, 502, 504, 510, 194, 600, 491, 398, 499, 164, 458, 499, 447, 184, 287, 333], [20, 440, 426, 33, 48, 333, 577, 434, 333], [126, 269, 504, 573, 330, 37, 155, 333, 504, 440, 329, 629, 383, 84, 366, 380, 244, 23, 532, 333, 504, 236, 188, 110, 449, 333], [12, 312, 269, 123, 37, 572, 333, 559, 499, 511, 66, 400, 200, 269, 353, 297, 127], [520, 261, 187, 610, 71, 261, 128, 610, 466, 499, 37, 43, 184, 592, 552, 504, 253, 584, 429], [509, 610, 566, 347, 306, 269, 188, 282, 259, 266, 479, 261, 475, 97, 27, 27, 151, 496, 579, 390, 410, 261, 68, 294, 374], [504, 440, 329, 22, 93, 552, 552, 552, 37, 115, 13, 595, 440, 47, 415, 479, 407, 252, 407], [171, 424, 538, 499, 37, 90, 534, 281, 312, 440, 554, 84, 376, 525, 491, 65, 530, 269, 438, 504, 331, 475, 13], [6, 267, 538, 269, 241, 241, 56, 610, 500, 552, 409, 141, 267, 268, 380, 469, 312, 364, 269, 492, 593, 261, 135], [503, 132, 5, 631, 579, 570, 570, 570, 216, 261, 504, 331, 300, 373], [88, 49, 552], [435, 370, 365, 167, 444, 35, 10, 177, 269, 472, 588, 552, 32, 610, 37, 17, 440, 426, 450, 253, 470, 171, 263, 490, 333, 76, 197, 20, 310, 557, 46, 541], [504, 331, 380, 552, 193, 504, 440, 329, 404, 513, 273, 484, 333, 261, 473, 221, 49, 64, 331, 380, 302, 514, 632, 304, 333, 255, 626, 70], [37, 29, 610, 37, 147, 203, 154, 272, 465, 174, 504, 564, 300, 295, 552, 465, 62, 118, 148, 504, 587, 49], [243, 526, 261, 189, 504, 550, 575, 131, 380, 363, 601, 556, 503, 171, 342, 278, 552], [432, 182, 84, 459, 261, 31], [504, 145, 32, 380, 369, 608, 269, 504, 420, 207, 32, 268, 149, 269, 313, 358, 579, 42, 261, 615, 51, 37, 6, 38, 499, 396], [624, 566, 49, 504, 493, 171, 390, 35, 115, 425, 261, 504, 440, 329, 267, 238, 337, 143, 27, 332, 333], [325, 347, 285, 422, 358, 430, 261, 391, 27, 159, 2, 65, 219, 613], [420, 120, 337, 47, 269, 123, 84, 299, 515, 623, 345, 81], [349, 407, 357, 579, 77, 504, 627, 407], [312, 168, 570, 570, 570, 407, 199, 470, 80, 272, 20, 43, 52, 517], [83, 108, 168, 48], [504, 440, 173, 264, 148, 37, 537, 625, 444, 193, 504, 510, 171, 75, 86], [226, 171, 82, 611, 195, 363, 549, 261, 632, 246, 309, 195, 84, 205, 312, 259, 169, 405], [594, 240, 14, 156, 333, 134], [414, 333, 316, 210, 579, 326, 111], [464, 263, 282, 347, 505, 579, 49], [504, 270, 143, 84, 50, 269, 504, 440, 329, 381, 261, 539, 444, 504, 575, 131, 385], [54, 178], [504, 531, 610, 380, 318], [464, 4, 140, 380, 209, 116, 401, 380, 119, 632, 288, 14, 261, 551, 418, 171, 298, 165, 261, 171, 393, 579, 632, 378, 90, 49, 336, 90, 415, 171, 523], [160, 407, 481, 493, 132, 394, 552, 552, 552, 503, 522, 407, 536], [168, 337, 171, 508, 333], [183, 440, 426, 533, 407, 470, 347, 591, 27, 204, 343, 49, 183, 440, 426, 171, 95, 196, 333], [478, 408, 171, 490, 621, 552, 315, 312, 440, 426, 629, 586, 261, 504, 440, 329, 337, 363, 489, 343, 408], [338], [614, 347, 361, 448, 260, 269, 148, 119, 272, 361], [504, 253, 427, 579, 576, 560], [171, 92, 431, 546, 4, 53, 440, 371, 347, 319, 49, 428], [201, 470, 132, 527, 9, 388, 49, 504, 41, 380, 60, 347, 171, 7, 261, 239, 265, 386, 499, 300, 1, 40, 247, 40, 487, 333, 269, 227], [465, 347, 289, 212, 35, 269, 347, 289, 296, 35, 269, 193, 123, 267, 367, 206, 347, 457, 276, 35, 465, 49], [224, 168, 95, 552, 491, 69, 548, 504, 355, 504, 154, 632, 102, 235, 188, 441, 337, 254, 630, 614, 224, 113, 380, 199, 261, 224, 162, 472], [281, 129, 504, 440, 329, 358, 579, 413, 499, 405, 333, 509, 39, 610, 380, 237, 275, 261, 422, 317, 497, 333, 504, 440, 419, 446, 283, 380, 37, 228, 156, 317, 621], [158, 269, 504, 510, 259, 344, 37, 402, 623, 589, 81, 479, 333, 94, 333], [84, 122, 237, 472], [504, 510, 142, 161], [417, 269, 347, 403, 300, 613, 21, 269, 261, 347, 301, 440, 371, 36, 112, 272, 407], [256, 130, 552], [597, 305, 407, 504, 491, 575, 460, 117, 220, 503, 610, 606, 407], [545, 78, 135, 499, 84, 212, 552], [423, 333, 27, 171, 144, 32, 10, 333], [253, 472, 150, 273, 447, 84, 277, 476, 491, 472, 34, 461, 229, 3, 504, 354, 171, 439, 269, 98, 208], [538, 320, 307, 13, 568, 552, 261, 453, 37, 348, 623, 395, 81], [157, 273, 347, 504, 440, 329, 610, 170, 491, 531, 272, 248, 579, 163, 491, 504, 289, 451, 610, 525, 380, 354, 337, 389, 35, 579, 18, 376, 372, 333], [542, 368, 168, 491, 519, 101, 312, 190, 272, 273, 96, 623, 32, 489, 405, 269, 428, 81, 193, 504, 411, 311, 584, 312, 614, 312, 384, 25, 504, 331, 204, 333], [504, 584, 347, 553, 286, 347, 441, 268, 23, 380, 280, 272, 148, 95, 333], [600, 464, 503, 528, 317, 506, 440, 371, 284, 191, 380, 179, 447, 619, 623, 253, 341, 198, 81, 193, 123, 171, 561, 390, 482, 504, 214], [562, 129, 73, 322, 168, 533, 552, 552, 552, 331, 312, 535], [166, 178, 261, 312, 104, 140, 43, 84, 74, 552], [275, 62, 282, 347, 211, 49, 49, 49, 570, 509, 540, 613, 407, 570], [135, 499, 96, 488, 380, 288, 610, 407, 629, 240, 491, 161, 337, 363, 23], [356, 215, 569, 261, 325, 616, 191, 499, 442, 333], [538, 79, 13, 380, 143, 347, 91, 358, 535], [109, 440, 426, 30, 49, 552, 49, 552, 193, 323, 49, 552, 491, 269, 224, 136, 171, 424, 317, 37, 138, 333, 224, 440, 426, 72, 468, 494, 312, 440, 426, 390, 333, 289, 188, 354, 632, 14, 49], [258, 463, 168, 454, 193, 172, 472], [629, 327, 171, 578, 567, 250, 175, 293, 325, 558, 333, 274, 617], [188, 583, 20, 610, 57, 26, 129, 37, 406, 440, 426, 37, 366]]\n"
     ]
    }
   ],
   "source": [
    "reverse_word_index = {v:k for k, v in words_index.items()}\n",
    "sequences = build_sequences(train_df['text'], reverse_word_index)\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_embeddings(data_path, index, build=True, save=True):\n",
    "    embeddings = np.zeros((len(index) + 1, 25))\n",
    "    reverse_index = {v: k for k, v in index.items()}\n",
    "    if build:\n",
    "        with open(data_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            for line in file:\n",
    "                tokens = line.split(\" \")\n",
    "                w = tokens[0]\n",
    "                zw = tokens[1:]\n",
    "                try:\n",
    "                    embeddings[reverse_index[w]] = np.array([float(zj) for zj in zw])\n",
    "                except KeyError :\n",
    "                    continue\n",
    "        if save:\n",
    "            with open (data_path, 'wb') as out:\n",
    "                pickle.dump(embeddings, out)\n",
    "    else:\n",
    "        with open(data_path,'rb') as out:\n",
    "            embeddings = pickle.load(out)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_test(data_path,nrows=None):\n",
    "    test=pd.read_csv(data_path,nrows = nrows)\n",
    "    test['text']=test['text'].astype(str)\n",
    "    test['sentiment']=test['sentiment'].astype(str)\n",
    "    return test\n",
    "test_df= read_test(\"C:\\\\Users\\\\PANGPENGHUI\\\\Desktop\\\\ESG\\\\ESG5BD\\\\machine_learning\\\\tweet_tp3\\\\test.csv\",nrows=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(635, 25)"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_matrix = read_embeddings(\"C:\\\\Users\\\\PANGPENGHUI\\\\Desktop\\\\ESG\\\\ESG5BD\\\\machine_learning\\\\tweet_tp3\\\\glove.twitter.27B.25d.txt\",index=words_index, build=False, save=False)\n",
    "embeddings_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 8)\n"
     ]
    }
   ],
   "source": [
    "T= 8\n",
    "x = pad_sequences(sequences,maxlen=T,padding=\"post\")\n",
    "\n",
    "\n",
    "print(x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_df[\"sentiement_index\"] = test_df[\"sentiment\"].map({\"neutral\":0,\"negative\":1,\"positive\":2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80,)"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "y = test_df['sentiement_index']\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x, y, stratify=y, test_size=0.2)\n",
    "X_train.shape\n",
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_LSTM=Sequential()\n",
    "model_LSTM.add(Embedding(635,100,input_length=T))\n",
    "model_LSTM.add(LSTM(64,dropout=0.2,return_sequences=True))\n",
    "model_LSTM.add(LSTM(32,dropout=0.3,return_sequences=False))\n",
    "model_LSTM.add(Dense(num_classes,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_40\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_31 (Embedding)     (None, 8, 100)            63500     \n",
      "_________________________________________________________________\n",
      "lstm_53 (LSTM)               (None, 8, 64)             42240     \n",
      "_________________________________________________________________\n",
      "lstm_54 (LSTM)               (None, 32)                12416     \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 118,255\n",
      "Trainable params: 118,255\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model_LSTM.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "80/80 [==============================] - 2s 23ms/step - loss: 0.7773 - accuracy: 0.7000\n",
      "Epoch 2/10\n",
      "80/80 [==============================] - 0s 586us/step - loss: 0.7160 - accuracy: 0.7250\n",
      "Epoch 3/10\n",
      "80/80 [==============================] - 0s 524us/step - loss: 0.6404 - accuracy: 0.7250\n",
      "Epoch 4/10\n",
      "80/80 [==============================] - 0s 573us/step - loss: 0.5591 - accuracy: 0.7375\n",
      "Epoch 5/10\n",
      "80/80 [==============================] - 0s 549us/step - loss: 0.4909 - accuracy: 0.7500\n",
      "Epoch 6/10\n",
      "80/80 [==============================] - 0s 549us/step - loss: 0.4149 - accuracy: 0.8250\n",
      "Epoch 7/10\n",
      "80/80 [==============================] - 0s 573us/step - loss: 0.3665 - accuracy: 0.9000\n",
      "Epoch 8/10\n",
      "80/80 [==============================] - 0s 549us/step - loss: 0.3097 - accuracy: 0.9500\n",
      "Epoch 9/10\n",
      "80/80 [==============================] - 0s 524us/step - loss: 0.2527 - accuracy: 0.9500\n",
      "Epoch 10/10\n",
      "80/80 [==============================] - 0s 573us/step - loss: 0.2116 - accuracy: 0.9875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1ac8fe70518>"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_LSTM.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_LSTM.fit(X_train,Y_train,batch_size=50, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.22490026 0.53285474 0.24224502]\n",
      " [0.6009771  0.04960369 0.34941927]\n",
      " [0.65724725 0.22837064 0.11438217]\n",
      " [0.68571454 0.02076572 0.29351968]\n",
      " [0.8063541  0.10113939 0.09250654]\n",
      " [0.2918426  0.45949364 0.24866377]\n",
      " [0.6073806  0.12549911 0.2671202 ]\n",
      " [0.66176623 0.01595982 0.32227394]\n",
      " [0.61313224 0.16402133 0.22284646]\n",
      " [0.62844515 0.07234304 0.29921174]\n",
      " [0.74614114 0.15771978 0.09613916]\n",
      " [0.45799434 0.08019165 0.46181393]\n",
      " [0.93636423 0.00598844 0.05764726]\n",
      " [0.63055176 0.02350519 0.3459431 ]\n",
      " [0.33633843 0.2945846  0.36907697]\n",
      " [0.87740934 0.08004987 0.04254085]\n",
      " [0.3525549  0.4209472  0.22649792]\n",
      " [0.13797563 0.74731207 0.11471224]\n",
      " [0.39025906 0.22135492 0.38838607]\n",
      " [0.62139    0.07746754 0.30114248]]\n"
     ]
    }
   ],
   "source": [
    "Y_pred_LSTM  = model_LSTM.predict(X_test)\n",
    "print(y_pred_LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6 2 1]\n",
      " [5 1 0]\n",
      " [3 1 1]]\n",
      "0.4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "print(confusion_matrix(Y_test, Y_pred_test))\n",
    "print(accuracy_score(Y_test, Y_pred_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
